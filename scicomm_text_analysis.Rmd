---
title: "SciComm project: Text Analysis"
output: html_notebook
---
```{r data setup, include=FALSE, message=FALSE, warning=FALSE}
library(pdftools)
library(tidyverse)
library(tidytext)
library(textdata) #for sentiment analysis
library(wordcloud)

data(stop_words) #loads dataset of stop words from the tidytext package

#stop word datasets for this dataset
physio_stop_words<-read.csv("~/Documents/GitHub/sci-comm-data-analysis/physio_stop_words.csv")
npb_stop_words<-tibble(word=c("assignment", "assignments", "question","questions","biweekly", "worksheets", "worksheet"))
                       
sc.text<-read.csv("~/Documents/projects/scicomm_analysis_Arik/data/raw_data_from_Arik/Science Communication - Group Assignement and responses.csv", na.strings = c(" ","","NA")) #biweekly text reflections into dataframe sc.text

sc<-read.csv(file="~/Documents/projects/scicomm_analysis_Arik/data/scicomm_data_joined.csv") #import joined scicomm data file

#rename columns 
colnames(sc.text)<-c("full_name","id", "sis_id","section","group","r1","r2","r3", "r4", "r5") 

```

```{r data manipulation, echo=FALSE}
#convert dataframe to long form so all text in one column 
sc.text<-sc.text %>% 
 select(id, group,r1:r5)%>%
  gather(biweekly_period, response, r1:r5) %>%
  drop_na(response) #after dropping NA, should be 864 reflections. 

#split text into lines (one line per line), but maintains the id / group / response week
library(tokenizers)
sc_responses_lines <- sc.text %>%
unnest_tokens(sentence, response, token= "lines") #this returns now 2668 observations 

#write to file for future analysis
write.csv(file="~/Desktop/scicomm_responses_by_line.csv", sc_responses_lines)

```

```{r create subset dataframes}
#merge all responses across all biweekly periods. 
#create dataframes for control text only , and treatment text only. 
control.text<- sc.text %>%
  filter(group =="Control") %>%
  select(response) %>%
  mutate_if(is.character, str_trim) #important: gets rid of white space rows 


#treatment text dataframe
treat.text<-sc.text %>%
  na.omit() %>%
  filter(group == "Treatment") %>%
  select(response) %>%
  mutate_if(is.character, str_trim) #gets rid of any white space rows
```

```{r control data text parsing}
#split text data by each new line (goes from ~400 lines to >2K)
control.text<-control.text %>% 
  filter(response != " ") %>% #omit rows with just a space that will throw off later
  mutate(response = strsplit(as.character(response), "\n")) %>%
  mutate(response = gsub("[[:digit:]]", "", response)) 

control.text <- data_frame(line = 1:427, text = control.text$response) #formatted by line 

#by word, using unnest_tokens
control.words <-control.text %>%
  unnest_tokens(word, text) 

#remove stop words
control.words <- control.words %>%
  anti_join(stop_words) %>% 
  anti_join(physio_stop_words) #returns all rows of text that do not match the stop_words dataset 

#sort words by counts (frequency)
control.counts<-control.words%>%
  count(word, sort=TRUE)

control.counts %>% 
  with(wordcloud(word, n, max.words = 10))
```

```{r treatment data text parsing}
#split text data by each new line (goes from ~400 lines to >2K)
trt.text<-treat.text %>% 
  filter(response != " ") %>% #omit rows with just a space that will throw off later
  mutate(response = strsplit(as.character(response), "\n")) %>%
  mutate(response = gsub("[[:digit:]]", "", response)) 

trt.text <- data_frame(line = 1:437, text = trt.text$response) #formatted by line 

#by word, using unnest_tokens
trt.words <-trt.text %>%
  unnest_tokens(word, text) 

#remove stop words
trt.words <- trt.words %>%
  anti_join(stop_words) %>% #returns all rows of text that do not match the stop_words dataset 
  anti_join(physio_stop_words)

#sort words by counts (frequency)
trt.counts<-trt.words%>%
  count(word, sort=TRUE)

#wordcloud
trt.counts %>% 
  with(wordcloud(word, n, max.words = 10, colors = "skyblue2"))

#bigram analysis (word pairs)
trt_bigrams <- trt.text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 %in% physio_stop_words$word) %>%
  filter(!word2 %in% physio_stop_words$word) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort=TRUE)

#bigram word cloud
trt_bigrams %>% 
  with(wordcloud(bigram, n, max.words = 100))

```
```{r final survey comments parsing}
#final survey comments analysis 

#extract control group comments to a dataframe
control.comments <- sc %>%
  filter(group == "Control") %>%
  select(comments) %>%
  na.omit()

#extract treatment group final comments
trt.comments <- sc %>%
  filter(group == "Treatment") %>%
  select(comments) %>%
  na.omit()
```

```{r control final comments, echo=FALSE}
#control comments text parsing
control.comments<-control.comments %>% 
  filter(comments != " ") %>% 
  filter(comments != "") %>% #omit rows with just a space that will throw off later
  mutate(comments = strsplit(as.character(comments), "\n"))

control.comments<- data_frame(line = 1:32, text = control.comments$comments) #formatted by line 

#word frequency list 
control.comments.words <-control.comments %>%
  unnest_tokens(word, text) %>% #parse into words 
  anti_join(stop_words) %>% #remove stop_words
  anti_join(npb_stop_words) %>% #remove course specific stop_wrods
  count(word, sort=TRUE) #return words in list of frequency

#wordcloud
control.comments.words %>% 
  with(wordcloud(word, n, max.words = 100), size=0.5)

```

```{r treatment final comments, echo=FALSE}
#control comments text parsing
trt.comments<-trt.comments %>% 
  filter(comments != " ") %>% 
  filter(comments != "") %>% #omit rows with just a space that will throw off later
  mutate(comments = strsplit(as.character(comments), "\n"))

trt.comments<- data_frame(line = 1:34, text = trt.comments$comments) #formatted by line 

#word frequency list 
trt.comments.words <-trt.comments %>%
  unnest_tokens(word, text) %>% #parse into words 
  anti_join(stop_words) %>% #remove stop_words
  anti_join(npb_stop_words) %>% #remove course specific stop_wrods
  count(word, sort=TRUE) #return words in list of frequency

#wordcloud
trt.comments.words %>% 
  with(wordcloud(word, n, max.words = 100), size=0.5)

```




```{r word clouds}
library(wordcloud)

control.words %>% 
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  with(wordcloud(word, n, max.words = 100))

trt.words %>% 
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  with(wordcloud(word, n, max.words = 100))
```

```{r}
#attempt at sentiment analysis  ###ISSUE: text data needs to be sorted before this. 
nrcneg <- get_sentiments("nrc") %>%
  filter(sentiment == "negative")

control.words %>%
  inner_join(nrcneg) %>%
  count(word, sort = TRUE)


```


